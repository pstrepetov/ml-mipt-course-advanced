{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13pL--6rycN3"
      },
      "source": [
        "## Homework02: Three headed network in PyTorch\n",
        "\n",
        "This notebook accompanies the [week02](https://github.com/girafe-ai/natural-language-processing/tree/master/week02_cnn_for_texts) practice session. Refer to that notebook for more comments.\n",
        "\n",
        "All the preprocessing is the same as in the classwork. *Including the data leakage in the train test split (it's still for bonus points).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P8zS7m-gycN5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import nltk\n",
        "import tqdm\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEFyv1ZVwYA5"
      },
      "source": [
        "If you have already downloaded the data on the Seminar, simply run through the next cells. Otherwise uncomment the next cell (and comment the another one ;)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xP0EN6h0wYA6",
        "outputId": "a17d66a3-687b-4100-b949-bbf5259943eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    17    0    17    0     0     59      0 --:--:-- --:--:-- --:--:--    59\n",
            "100   342  100   342    0     0    420      0 --:--:-- --:--:-- --:--:--   420\n",
            "100  119M  100  119M    0     0  38.6M      0  0:00:03  0:00:03 --:--:-- 68.2M\n",
            "Train_rev1.csv\n",
            "--2023-05-11 19:52:04--  https://raw.githubusercontent.com/girafe-ai/natural-language-processing/22f_msai/homeworks/assignment02_three_headed_network/network.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1469 (1.4K) [text/plain]\n",
            "Saving to: ‘network.py’\n",
            "\n",
            "network.py          100%[===================>]   1.43K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-11 19:52:04 (25.8 MB/s) - ‘network.py’ saved [1469/1469]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# uncomment and run this cell, if you don't have data locally yet.\n",
        "\n",
        "!curl -L \"https://www.dropbox.com/s/5msc5ix7ndyba10/Train_rev1.csv.tar.gz?dl=1\" -o Train_rev1.csv.tar.gz\n",
        "!tar -xvzf ./Train_rev1.csv.tar.gz\n",
        "\n",
        "data = pd.read_csv(\"./Train_rev1.csv\", index_col=None)\n",
        "\n",
        "!wget https://raw.githubusercontent.com/pstrepetov/ml-mipt-course-advanced/main/assignment02_three_headed_network/network.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwN72gd4ycOA"
      },
      "outputs": [],
      "source": [
        "# run this cell if you have downloaded the dataset on the seminar\n",
        "#data = pd.read_csv(\"../../week02_CNN_n_Vanishing_gradient/Train_rev1.csv\", index_col=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UuuKIKfrycOH"
      },
      "outputs": [],
      "source": [
        "data['Log1pSalary'] = np.log1p(data['SalaryNormalized']).astype('float32')\n",
        "text_columns = [\"Title\", \"FullDescription\"]\n",
        "categorical_columns = [\"Category\", \"Company\", \"LocationNormalized\", \"ContractType\", \"ContractTime\"]\n",
        "target_column = \"Log1pSalary\"\n",
        "\n",
        "data[categorical_columns] = data[categorical_columns].fillna('NaN') # cast missing values to string \"NaN\"\n",
        "\n",
        "data.sample(3)\n",
        "\n",
        "\n",
        "data_for_autotest = data[-5000:]\n",
        "data = data[:-5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUWkpd7PycOQ",
        "outputId": "9a02ffd4-dbb6-4aae-ccb1-b87d05acd1b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized:\n",
            "2         mathematical modeller / simulation analyst / o...\n",
            "100002    a successful and high achieving specialist sch...\n",
            "200002    web designer html , css , javascript , photosh...\n",
            "Name: FullDescription, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "239768it [00:24, 9962.82it/s] \n"
          ]
        }
      ],
      "source": [
        "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
        "# see task above\n",
        "def normalize(text):\n",
        "    text = str(text).lower()\n",
        "    return ' '.join(tokenizer.tokenize(text))\n",
        "    \n",
        "data[text_columns] = data[text_columns].applymap(normalize)\n",
        "\n",
        "print(\"Tokenized:\")\n",
        "print(data[\"FullDescription\"][2::100000])\n",
        "assert data[\"FullDescription\"][2][:50] == 'mathematical modeller / simulation analyst / opera'\n",
        "assert data[\"Title\"][54321] == 'international digital account manager ( german )'\n",
        "\n",
        "# Count how many times does each token occur in both \"Title\" and \"FullDescription\" in total\n",
        "# build a dictionary { token -> it's count }\n",
        "from collections import Counter\n",
        "from tqdm import tqdm as tqdm\n",
        "\n",
        "token_counts = Counter()# <YOUR CODE HERE>\n",
        "for _, row in tqdm(data[text_columns].iterrows()):\n",
        "    for string in row:\n",
        "        token_counts.update(string.split())\n",
        "\n",
        "# hint: you may or may not want to use collections.Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z21nh61VwYA_",
        "outputId": "9e29cffd-d451-4dfe-c3a9-456ff10be2cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2598827"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "token_counts.most_common(1)[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiOWbc15ycOb",
        "outputId": "63ce1c73-92e3-461c-975e-5d42b4cf3467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique tokens : 201127\n",
            "('and', 2598827)\n",
            "('.', 2471477)\n",
            "(',', 2266256)\n",
            "('the', 2036428)\n",
            "('to', 1977039)\n",
            "...\n",
            "('dbms_stats', 1)\n",
            "('dbms_output', 1)\n",
            "('dbms_job', 1)\n",
            "Correct!\n",
            "Vocabulary size: 33795\n",
            "Correct!\n",
            "Correct!\n"
          ]
        }
      ],
      "source": [
        "print(\"Total unique tokens :\", len(token_counts))\n",
        "print('\\n'.join(map(str, token_counts.most_common(n=5))))\n",
        "print('...')\n",
        "print('\\n'.join(map(str, token_counts.most_common()[-3:])))\n",
        "\n",
        "assert token_counts.most_common(1)[0][1] in  range(2500000, 2700000)\n",
        "assert len(token_counts) in range(200000, 210000)\n",
        "print('Correct!')\n",
        "\n",
        "min_count = 10\n",
        "\n",
        "# tokens from token_counts keys that had at least min_count occurrences throughout the dataset\n",
        "tokens = [token for token, count in token_counts.items() if count >= min_count]# <YOUR CODE HERE>\n",
        "# Add a special tokens for unknown and empty words\n",
        "UNK, PAD = \"UNK\", \"PAD\"\n",
        "tokens = [UNK, PAD] + sorted(tokens)\n",
        "print(\"Vocabulary size:\", len(tokens))\n",
        "\n",
        "assert type(tokens) == list\n",
        "assert len(tokens) in range(32000, 35000)\n",
        "assert 'me' in tokens\n",
        "assert UNK in tokens\n",
        "print(\"Correct!\")\n",
        "\n",
        "token_to_id = {token: idx for idx, token in enumerate(tokens)}\n",
        "assert isinstance(token_to_id, dict)\n",
        "assert len(token_to_id) == len(tokens)\n",
        "for tok in tokens:\n",
        "    assert tokens[token_to_id[tok]] == tok\n",
        "\n",
        "print(\"Correct!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JEsLeBjVycOw"
      },
      "outputs": [],
      "source": [
        "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
        "\n",
        "def as_matrix(sequences, max_len=None):\n",
        "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
        "    if isinstance(sequences[0], str):\n",
        "        sequences = list(map(str.split, sequences))\n",
        "        \n",
        "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
        "    \n",
        "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
        "    for i,seq in enumerate(sequences):\n",
        "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
        "        matrix[i, :len(row_ix)] = row_ix\n",
        "    \n",
        "    return matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiBlPkdKycOy",
        "outputId": "778b0061-dbd9-44c1-f639-daa2bc857047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lines:\n",
            "engineering systems analyst\n",
            "hr assistant\n",
            "senior ec & i engineer\n",
            "\n",
            "Matrix:\n",
            "[[10705 29830  2143     1     1]\n",
            " [14875  2817     1     1     1]\n",
            " [27345 10107    15 15069 10702]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Lines:\")\n",
        "print('\\n'.join(data[\"Title\"][::100000].values), end='\\n\\n')\n",
        "print(\"Matrix:\")\n",
        "print(as_matrix(data[\"Title\"][::100000]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "DpOlBp7ZycO6",
        "outputId": "e888cb46-4fd1-4d66-f994-477113f46018"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DictVectorizer(dtype=<class 'numpy.float32'>, sparse=False)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DictVectorizer(dtype=&lt;class &#x27;numpy.float32&#x27;&gt;, sparse=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DictVectorizer</label><div class=\"sk-toggleable__content\"><pre>DictVectorizer(dtype=&lt;class &#x27;numpy.float32&#x27;&gt;, sparse=False)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "# we only consider top-1k most frequent companies to minimize memory usage\n",
        "top_companies, top_counts = zip(*Counter(data['Company']).most_common(1000))\n",
        "recognized_companies = set(top_companies)\n",
        "data[\"Company\"] = data[\"Company\"].apply(lambda comp: comp if comp in recognized_companies else \"Other\")\n",
        "\n",
        "categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False)\n",
        "categorical_vectorizer.fit(data[categorical_columns].apply(dict, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk4jmtAYycO8"
      },
      "source": [
        "### The deep learning part\n",
        "\n",
        "Once we've learned to tokenize the data, let's design a machine learning experiment.\n",
        "\n",
        "As before, we won't focus too much on validation, opting for a simple train-test split.\n",
        "\n",
        "__To be completely rigorous,__ we've comitted a small crime here: we used the whole data for tokenization and vocabulary building. A more strict way would be to do that part on training set only. You may want to do that and measure the magnitude of changes.\n",
        "\n",
        "\n",
        "#### Here comes the simple one-headed network from the seminar. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TngLcWA0ycO_",
        "outputId": "5c55a735-20df-4f78-c8a8-e6d52d45c314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size =  191814\n",
            "Validation size =  47954\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42)\n",
        "data_train.index = range(len(data_train))\n",
        "data_val.index = range(len(data_val))\n",
        "\n",
        "print(\"Train size = \", len(data_train))\n",
        "print(\"Validation size = \", len(data_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2PXuKgOSycPB"
      },
      "outputs": [],
      "source": [
        "def make_batch(data, max_len=None, word_dropout=0):\n",
        "    \"\"\"\n",
        "    Creates a keras-friendly dict from the batch data.\n",
        "    :param word_dropout: replaces token index with UNK_IX with this probability\n",
        "    :returns: a dict with {'title' : int64[batch, title_max_len]\n",
        "    \"\"\"\n",
        "    batch = {}\n",
        "    batch[\"Title\"] = as_matrix(data[\"Title\"].values, max_len)\n",
        "    batch[\"FullDescription\"] = as_matrix(data[\"FullDescription\"].values, max_len)\n",
        "    batch['Categorical'] = categorical_vectorizer.transform(data[categorical_columns].apply(dict, axis=1))\n",
        "    \n",
        "    if word_dropout != 0:\n",
        "        batch[\"FullDescription\"] = apply_word_dropout(batch[\"FullDescription\"], 1. - word_dropout)\n",
        "    \n",
        "    if target_column in data.columns:\n",
        "        batch[target_column] = data[target_column].values\n",
        "    \n",
        "    return batch\n",
        "\n",
        "def apply_word_dropout(matrix, keep_prop, replace_with=UNK_IX, pad_ix=PAD_IX,):\n",
        "    dropout_mask = np.random.choice(2, np.shape(matrix), p=[keep_prop, 1 - keep_prop])\n",
        "    dropout_mask &= matrix != pad_ix\n",
        "    return np.choose(dropout_mask, [matrix, np.full_like(matrix, replace_with)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "I6LpEQf0ycPD"
      },
      "outputs": [],
      "source": [
        "a = make_batch(data_train[:3], max_len=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aCU4bo5wYBD"
      },
      "source": [
        "But to start with let's build the simple model using only the part of the data. Let's create the baseline solution using only the description part (so it should definetely fit into the Sequential model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6cnAjl62wYBD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eb8MnA-9wYBD"
      },
      "outputs": [],
      "source": [
        "# You will need these to make it simple\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "class Reorder(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.permute((0, 2, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R54rAQtrwYBE"
      },
      "source": [
        "To generate minibatches we will use simple pyton generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jVFbZ9t0wYBE"
      },
      "outputs": [],
      "source": [
        "def iterate_minibatches(data, batch_size=256, shuffle=True, cycle=False, **kwargs):\n",
        "    \"\"\" iterates minibatches of data in random order \"\"\"\n",
        "    while True:\n",
        "        indices = np.arange(len(data))\n",
        "        if shuffle:\n",
        "            indices = np.random.permutation(indices)\n",
        "\n",
        "        for start in range(0, len(indices), batch_size):\n",
        "            batch = make_batch(data.iloc[indices[start : start + batch_size]], **kwargs)\n",
        "            target = batch.pop(target_column)\n",
        "            yield batch, target\n",
        "        \n",
        "        if not cycle: break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CQn7ZXeTwYBE"
      },
      "outputs": [],
      "source": [
        "iterator = iterate_minibatches(data_train, 3)\n",
        "batch, target = next(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rExA6koOwYBE"
      },
      "outputs": [],
      "source": [
        "# Here is some startup code:\n",
        "n_tokens=len(tokens)\n",
        "n_cat_features=len(categorical_vectorizer.vocabulary_)\n",
        "hid_size=64\n",
        "simple_model = nn.Sequential()\n",
        "\n",
        "simple_model.add_module('emb', nn.Embedding(num_embeddings=n_tokens, embedding_dim=hid_size))\n",
        "simple_model.add_module('reorder', Reorder())\n",
        "simple_model.add_module('conv1', nn.Conv1d(\n",
        "    in_channels=hid_size,\n",
        "    out_channels=hid_size,\n",
        "    kernel_size=2)\n",
        "                       )\n",
        "simple_model.add_module('relu1', nn.ReLU())\n",
        "simple_model.add_module('adapt_avg_pool', nn.AdaptiveAvgPool1d(output_size=1))\n",
        "simple_model.add_module('flatten1', Flatten())\n",
        "simple_model.add_module('linear1', nn.Linear(in_features=hid_size, out_features=1))\n",
        "# <YOUR CODE HERE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "r16cPATQwYBF",
        "outputId": "e6bcf68b-b56d-4fce-e6ae-fd866550c958",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Title': array([[10364, 15879, 30146,  2166, 10711,     1],\n",
              "        [17666, 25112,  7252,     1,     1,     1],\n",
              "        [13520,   195, 16837, 10491, 28294, 10702]], dtype=int32),\n",
              " 'FullDescription': array([[31823,  2166,  6712, 26330, 12466,   891, 10373, 10364, 10711,\n",
              "         30146, 15879, 10711, 30146, 12466, 27956,  2594, 30411, 21058,\n",
              "         10065,  4353,  7338,  2166, 22755, 16670,   167,  7338, 24820,\n",
              "          8803, 21556, 11453,   167,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1],\n",
              "        [17666, 25112,  7252, 20386,  6347, 16289,  2120, 11082,  2166,\n",
              "         15509, 25112,  4466, 30407, 24252,   965, 12177,  6261, 25112,\n",
              "         27445, 30762, 17498, 12172,  1297, 30411, 21058, 32907, 25341,\n",
              "           167, 16378, 14083,  4841,   965, 28331,  2166, 29406, 25705,\n",
              "         22048,   965, 18207, 22737,  2166, 16289, 31923,  5016, 18781,\n",
              "         16006,   156, 20462,  2166, 15506,  2166, 25342, 17498, 12172,\n",
              "          2662, 30422, 28320, 25133, 22377,   167, 30411, 12168, 14083,\n",
              "          2040, 23153, 30762, 12913, 11401,  1297, 30411, 31523, 15402,\n",
              "           461,  2166, 21416, 20472,  7678, 21405, 16416, 25112,   156,\n",
              "          7251,  2166, 27115, 27463,   167, 30762,  1109, 30512, 13717,\n",
              "         32718,  2545, 18235, 30762, 29922, 30762, 11458, 17666, 25112,\n",
              "          7253,  2166, 18689, 18235, 30762, 16729,   965, 14446, 13717,\n",
              "         22731,  4938,  2166, 29954, 16079, 16416, 18207, 30311, 29402,\n",
              "           167, 33635, 33079, 14109,  2892, 17622,   307, 19947, 11453,\n",
              "          4978, 15143, 19981,   156, 15143, 33331, 33209,   965, 17666,\n",
              "          2166,   195, 21784, 24024, 27463, 25112, 10866,  2166, 30455,\n",
              "         14118, 13446, 17203, 21405, 14850, 24024, 25112, 33352,   167,\n",
              "         33635, 33079,  3607, 14009, 33331,   156, 12397,   156, 23425,\n",
              "           156,   965, 13446,  6810,  2166,  3607, 31923, 30762, 20673,\n",
              "          2166,  4835, 29406,  4938, 25449,   167, 33635, 33079,  1968,\n",
              "         14109,   965, 24240, 30985, 25078, 15402, 33642, 12023,  2166,\n",
              "          3607,  1048, 30762,  8756, 33642, 29402,   167, 15402,  1389,\n",
              "         30762,     0, 32718,  2545,  1973, 27195, 18689, 30762, 11081,\n",
              "          2166,  4828, 30103,   167, 18132, 16325,  2120, 16362,   167,\n",
              "         32718,  2545,     0, 15972, 15402,  2385,  2594, 18192,   156,\n",
              "          4635,   156, 17649,   156,  4030,   156, 18049,  2166, 27015,\n",
              "           167, 30512, 16658, 32637, 21870, 23459,  2662, 33468,   167,\n",
              "         30895,   167,  6681,   195, 16679,   195,     0,    80,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1],\n",
              "        [ 2545, 33635,   965, 13520,   195, 16837, 10491, 28294, 10702,\n",
              "         18235, 30762,  6692, 33642, 22405, 12466, 10491, 29830, 33198,\n",
              "         33374,  6261,  3115, 30160,   927, 30512, 16289,  2120, 15142,\n",
              "         26324, 12466, 25009, 13520, 21784, 16837, 10491, 28294, 10702,\n",
              "         30762, 16729, 28365, 21405, 30411,  3837,  4447, 15402, 30411,\n",
              "          4938,  8998, 10491,  7366, 29830, 12466, 30847, 21405, 30411,\n",
              "         24786,  5428,  2662,   965, 16837, 10491, 28294, 10702, 33635,\n",
              "         18065, 25001, 21556, 30411, 16658, 31046,   156,  8998, 11491,\n",
              "         15402, 10491, 28294,   156,  8341,  1287,  2166,  7366, 29830,\n",
              "          2662,   965, 16837, 10491, 28294, 10702, 33635, 18065, 14109,\n",
              "         12351,  5030, 24077, 28011, 12466, 10491, 29830,   156, 23678,\n",
              "         33198,  2120, 31641, 21405,  2776, 24077,   167, 16837,   195,\n",
              "         13520, 10491, 28294, 10702, 25729,   891,  5030, 24077, 12466,\n",
              "         10491, 29830, 16837,   195, 13520, 10491, 28294, 10702,  8911,\n",
              "           891,  8536,   156, 11124,   156,  5196,   156, 32117, 30842,\n",
              "           156,  3115, 19499,   156, 12714, 23518,  2532, 23979, 20110,\n",
              "            80,   195,    80,   195,    80,   195,    80, 22746,  9018,\n",
              "          5196,   156, 26491,    80,  8260,   156,  1374,   156, 28607,\n",
              "           156,     0,   156, 18989,   156, 27904,   156, 19721,  5030,\n",
              "           156, 10700,  7366,   156,  3115,  2389, 25861,  2662,   965,\n",
              "         13520,   195, 16837, 10491, 28294, 10702,   891,  9000, 21405,\n",
              "         10491,  7366, 29830, 12466,  3115, 30160, 33306, 15402,   965,\n",
              "         30080, 30762,  8646,  8894,  2166,  7020,   156, 15353,  2166,\n",
              "         30343, 21405, 31826, 10491, 28294,  9000, 21405, 14446, 17758,\n",
              "          3115,  2389, 11184, 21405, 20697, 30157, 12466, 15394, 28294,\n",
              "          9000, 23975, 23415,   891, 16837,   195, 13520, 10491, 28294,\n",
              "         10702, 15142, 12466,  2120, 10491, 28294, 10702, 33198,   307,\n",
              "         33591, 11453, 21784,   965, 25009, 13520, 18235, 30762, 11517,\n",
              "          8164, 10191, 10491, 29830, 18132,   891, 29617,   156,  6002,\n",
              "           156, 13776, 26682,   891,    80, 16873, 13446,  3771, 22170,\n",
              "         28011,   891, 10491, 29830,   156,  5030, 24077,   156,  2776,\n",
              "           156,  7366, 29830, 21084, 29591,   927, 13315, 16589, 19110,\n",
              "           965,  5124,  2892, 10792, 25112, 18359]], dtype=int32),\n",
              " 'Categorical': array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6ZLeQOgwYBF"
      },
      "source": [
        "__Remember!__ We are working with regression problem and predicting only one number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tCVqJA7NwYBF",
        "outputId": "ff7531b1-9dc1-442c-ed3e-f51672bb8a42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1050],\n",
              "        [-0.0062],\n",
              "        [ 0.0029]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Try this to check your model. `torch.long` tensors are required for nn.Embedding layers.\n",
        "simple_model(torch.tensor(batch['FullDescription'], dtype=torch.long))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Jp3eiNCqwYBG",
        "outputId": "7aad8139-da6a-490f-9d5d-3b731f75cb84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 294)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "batch['FullDescription'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWv1A9EEwYBG"
      },
      "source": [
        "And now simple training pipeline (it's commented because we've already done that in class. No need to do it again)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wea9h42bwYBG"
      },
      "outputs": [],
      "source": [
        "# from IPython.display import clear_output\n",
        "# from random import sample\n",
        "\n",
        "# epochs = 1\n",
        "\n",
        "# model = simple_model\n",
        "# opt = torch.optim.Adam(model.parameters())\n",
        "# loss_func = nn.MSELoss()\n",
        "\n",
        "# history = []\n",
        "# for epoch_num in range(epochs):\n",
        "#     for idx, (batch, target) in enumerate(iterate_minibatches(data_train)):\n",
        "#         # Preprocessing the batch data and target\n",
        "#         batch = torch.tensor(batch['FullDescription'], dtype=torch.long)\n",
        "\n",
        "#         target = torch.tensor(target)\n",
        "\n",
        "\n",
        "#         predictions = model(batch)\n",
        "#         predictions = predictions.view(predictions.size(0))\n",
        "\n",
        "#         loss = loss_func(predictions, target)# <YOUR CODE HERE>\n",
        "\n",
        "#         # train with backprop\n",
        "#         loss.backward()\n",
        "#         opt.step()\n",
        "#         opt.zero_grad()\n",
        "#         # <YOUR CODE HERE>\n",
        "\n",
        "#         history.append(loss.data.numpy())\n",
        "#         if (idx+1)%10==0:\n",
        "#             clear_output(True)\n",
        "#             plt.plot(history,label='loss')\n",
        "#             plt.legend()\n",
        "#             plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaSCgAfFwYBH"
      },
      "source": [
        "### Actual homework starts here\n",
        "__Your ultimate task is to code the three headed network described on the picture below.__ \n",
        "To make it closer to the real world, please store the network code in file `network.py` in this directory. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eI5h9UMycPF"
      },
      "source": [
        "#### Architecture\n",
        "\n",
        "Our main model consists of three branches:\n",
        "* Title encoder\n",
        "* Description encoder\n",
        "* Categorical features encoder\n",
        "\n",
        "We will then feed all 3 branches into one common network that predicts salary.\n",
        "\n",
        "<img src=\"https://github.com/yandexdataschool/nlp_course/raw/master/resources/w2_conv_arch.png\" width=600px>\n",
        "\n",
        "This clearly doesn't fit into PyTorch __Sequential__ interface. To build such a network, one will have to use [__PyTorch nn.Module API__](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3GBIaPpYwYBH"
      },
      "outputs": [],
      "source": [
        "import network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "bl8NGgwUwYBH",
        "outputId": "453895a1-a2a8-4743-ee74-340cc6d86709",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'network' from '/content/network.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# Re-run this cell if you updated the file with network source code\n",
        "import imp\n",
        "imp.reload(network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "uwRgtIWkwYBI"
      },
      "outputs": [],
      "source": [
        "hid_size = 64\n",
        "\n",
        "model = network.ThreeInputsNet(\n",
        "    n_tokens=len(tokens),\n",
        "    n_cat_features=len(categorical_vectorizer.vocabulary_),\n",
        "\n",
        "    # this parameter defines the number of the inputs in the layer,\n",
        "    # which stands after the concatenation. In should be found out by you.\n",
        "    concat_number_of_features=hid_size*2 + 20\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "JtcIWRPewYBI"
      },
      "outputs": [],
      "source": [
        "testing_batch, _ = next(iterate_minibatches(data_train, 3))\n",
        "testing_batch = [\n",
        "    torch.tensor(testing_batch['Title'], dtype=torch.long),\n",
        "    torch.tensor(testing_batch['FullDescription'], dtype=torch.long),\n",
        "    torch.tensor(testing_batch['Categorical'])\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "_ahIiBWpwYBI",
        "outputId": "064b723f-075f-4149-c6c6-f4ad60abefd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seems fine!\n"
          ]
        }
      ],
      "source": [
        "assert model(testing_batch).shape == torch.Size([3, 1])\n",
        "assert model(testing_batch).dtype == torch.float32\n",
        "print('Seems fine!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30ulNtdhwYBI"
      },
      "source": [
        "Now train the network for a while (100 batches would be fine)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "BB7hFv8IwYBJ",
        "outputId": "d0e6c147-a4dd-4ee3-834f-8c6449411079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "from random import sample\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "history = []\n",
        "for epoch_num in range(epochs):\n",
        "    for idx, (batch, target) in enumerate(iterate_minibatches(data_train)):\n",
        "        # Preprocessing the batch data and target\n",
        "\n",
        "        batch = [\n",
        "            torch.tensor(batch['Title'], dtype=torch.long),\n",
        "            torch.tensor(batch['FullDescription'], dtype=torch.long),\n",
        "            torch.tensor(batch['Categorical'])\n",
        "            ]\n",
        "\n",
        "        target = torch.tensor(target)\n",
        "\n",
        "\n",
        "        predictions = model(batch)\n",
        "        predictions = predictions.view(predictions.size(0))\n",
        "\n",
        "        loss = loss_func(predictions, target)\n",
        "\n",
        "        # train with backprop\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        history.append(loss.data.numpy())\n",
        "        if (idx+1)%10==0:\n",
        "            clear_output(True)\n",
        "            plt.plot(history,label='loss')\n",
        "            plt.legend()\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3quhWR7wYBJ"
      },
      "source": [
        "Now, to evaluate the model it can be switched to `eval` state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "SepHFYpYwYBJ",
        "outputId": "8ee21f89-c005-491a-c3a0-156e19cd5dc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ThreeInputsNet(\n",
              "  (title_emb): Embedding(33795, 64)\n",
              "  (title_encoder): Sequential(\n",
              "    (0): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n",
              "    (1): ReLU()\n",
              "    (2): AdaptiveAvgPool1d(output_size=1)\n",
              "    (3): Flatten()\n",
              "  )\n",
              "  (full_emb): Embedding(33795, 64)\n",
              "  (full_encoder): Sequential(\n",
              "    (0): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n",
              "    (1): ReLU()\n",
              "    (2): AdaptiveAvgPool1d(output_size=1)\n",
              "    (3): Flatten()\n",
              "  )\n",
              "  (category_out): Sequential(\n",
              "    (0): Linear(in_features=3746, out_features=600, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=600, out_features=100, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=100, out_features=20, bias=True)\n",
              "  )\n",
              "  (inter_dense): Linear(in_features=148, out_features=128, bias=True)\n",
              "  (final_dense): Linear(in_features=128, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "k94C5LNgwYBJ"
      },
      "outputs": [],
      "source": [
        "def generate_submission(model, data, batch_size=256, name=\"\", three_inputs_mode=True, **kw):\n",
        "    squared_error = abs_error = num_samples = 0.0\n",
        "    output_list = []\n",
        "    for batch_x, batch_y in tqdm(iterate_minibatches(data, batch_size=batch_size, shuffle=False, **kw)):\n",
        "        if three_inputs_mode:\n",
        "            batch = [\n",
        "                torch.tensor(batch_x['Title'], dtype=torch.long),\n",
        "                torch.tensor(batch_x['FullDescription'], dtype=torch.long),\n",
        "                torch.tensor(batch_x['Categorical'])\n",
        "            ]\n",
        "        else:\n",
        "            batch = torch.tensor(batch_x['FullDescription'], dtype=torch.long)\n",
        "\n",
        "        batch_pred = model(batch)[:, 0].detach().numpy()\n",
        "        \n",
        "        output_list.append((list(batch_pred), list(batch_y)))\n",
        "        \n",
        "        squared_error += np.sum(np.square(batch_pred - batch_y))\n",
        "        abs_error += np.sum(np.abs(batch_pred - batch_y))\n",
        "        num_samples += len(batch_y)\n",
        "    print(\"%s results:\" % (name or \"\"))\n",
        "    print(\"Mean square error: %.5f\" % (squared_error / num_samples))\n",
        "    print(\"Mean absolute error: %.5f\" % (abs_error / num_samples))\n",
        "    \n",
        "\n",
        "    batch_pred = [c for x in output_list for c in x[0]]\n",
        "    batch_y = [c for x in output_list for c in x[1]]\n",
        "    output_df = pd.DataFrame(list(zip(batch_pred, batch_y)), columns=['batch_pred', 'batch_y'])\n",
        "    output_df.to_csv('submission.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "8O57JC1LwYBJ",
        "outputId": "7af2af6c-ee0d-4a77-c949-634ec6356525",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "20it [00:05,  3.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission results:\n",
            "Mean square error: 0.14248\n",
            "Mean absolute error: 0.29385\n",
            "Submission file generated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "generate_submission(model, data_for_autotest, name='Submission')\n",
        "print('Submission file generated')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNJ_ZBnxwYBJ"
      },
      "source": [
        "__Both the notebook and the `.py` file are required to submit this homework.__"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
